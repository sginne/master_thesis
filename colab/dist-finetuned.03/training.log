2024-09-07 15:06:45,869 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,870 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(119548, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0-5): 6 x TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
              (activation): GELUActivation()
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 128, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=256, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2024-09-07 15:06:45,871 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,872 Corpus: 7886 train + 876 dev + 4045 test sentences
2024-09-07 15:06:45,873 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,873 Train:  8762 sentences
2024-09-07 15:06:45,874         (train_with_dev=True, train_with_test=False)
2024-09-07 15:06:45,874 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,875 Training Params:
2024-09-07 15:06:45,876  - learning_rate: "0.01" 
2024-09-07 15:06:45,877  - mini_batch_size: "1"
2024-09-07 15:06:45,878  - max_epochs: "2"
2024-09-07 15:06:45,878  - shuffle: "True"
2024-09-07 15:06:45,880 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,880 Plugins:
2024-09-07 15:06:45,881  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2024-09-07 15:06:45,881 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,882 Final evaluation on model from best epoch (best-model.pt)
2024-09-07 15:06:45,883  - metric: "('micro avg', 'f1-score')"
2024-09-07 15:06:45,883 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,884 Computation:
2024-09-07 15:06:45,884  - compute on device: cuda:0
2024-09-07 15:06:45,885  - embedding storage: cpu
2024-09-07 15:06:45,886 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,887 Model training base path: "dist-finetuned.03"
2024-09-07 15:06:45,887 ----------------------------------------------------------------------------------------------------
2024-09-07 15:06:45,888 ----------------------------------------------------------------------------------------------------
2024-09-07 15:07:44,263 epoch 1 - iter 876/8762 - loss 0.31395019 - time (sec): 58.36 - samples/sec: 272.88 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:08:35,404 epoch 1 - iter 1752/8762 - loss 0.26524104 - time (sec): 109.52 - samples/sec: 295.37 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:09:26,812 epoch 1 - iter 2628/8762 - loss 0.22162247 - time (sec): 160.92 - samples/sec: 297.57 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:10:17,936 epoch 1 - iter 3504/8762 - loss 0.19414913 - time (sec): 212.05 - samples/sec: 299.16 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:11:08,888 epoch 1 - iter 4380/8762 - loss 0.18300235 - time (sec): 263.00 - samples/sec: 300.62 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:12:00,815 epoch 1 - iter 5256/8762 - loss 0.17936442 - time (sec): 314.93 - samples/sec: 301.43 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:12:52,235 epoch 1 - iter 6132/8762 - loss 0.17246925 - time (sec): 366.35 - samples/sec: 303.03 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:13:43,484 epoch 1 - iter 7008/8762 - loss 0.16517164 - time (sec): 417.59 - samples/sec: 305.07 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:14:34,251 epoch 1 - iter 7884/8762 - loss 0.15590375 - time (sec): 468.36 - samples/sec: 304.97 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:15:25,167 epoch 1 - iter 8760/8762 - loss 0.15188179 - time (sec): 519.28 - samples/sec: 305.57 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:15:25,308 ----------------------------------------------------------------------------------------------------
2024-09-07 15:15:25,309 EPOCH 1 done: loss 0.1518 - lr: 0.010000
2024-09-07 15:16:31,311 TEST : loss 0.13795699179172516 - f1-score (micro avg)  0.6776
2024-09-07 15:16:31,527  - 0 epochs without improvement
2024-09-07 15:16:31,540 ----------------------------------------------------------------------------------------------------
2024-09-07 15:17:22,559 epoch 2 - iter 876/8762 - loss 0.10247964 - time (sec): 51.02 - samples/sec: 299.11 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:18:13,451 epoch 2 - iter 1752/8762 - loss 0.09949893 - time (sec): 101.91 - samples/sec: 306.99 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:19:03,673 epoch 2 - iter 2628/8762 - loss 0.10312148 - time (sec): 152.13 - samples/sec: 312.62 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:19:55,044 epoch 2 - iter 3504/8762 - loss 0.09920466 - time (sec): 203.50 - samples/sec: 311.99 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:20:46,361 epoch 2 - iter 4380/8762 - loss 0.09891934 - time (sec): 254.82 - samples/sec: 310.09 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:21:37,976 epoch 2 - iter 5256/8762 - loss 0.09955118 - time (sec): 306.43 - samples/sec: 310.14 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:22:28,646 epoch 2 - iter 6132/8762 - loss 0.09913963 - time (sec): 357.10 - samples/sec: 312.08 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:23:19,846 epoch 2 - iter 7008/8762 - loss 0.09632479 - time (sec): 408.30 - samples/sec: 312.36 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:24:10,354 epoch 2 - iter 7884/8762 - loss 0.09445449 - time (sec): 458.81 - samples/sec: 311.97 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:25:01,738 epoch 2 - iter 8760/8762 - loss 0.09292120 - time (sec): 510.20 - samples/sec: 311.01 - lr: 0.010000 - momentum: 0.000000
2024-09-07 15:25:01,881 ----------------------------------------------------------------------------------------------------
2024-09-07 15:25:01,882 EPOCH 2 done: loss 0.0929 - lr: 0.010000
2024-09-07 15:25:08,998 TEST : loss 0.14089982211589813 - f1-score (micro avg)  0.6766
2024-09-07 15:25:09,207  - 0 epochs without improvement
2024-09-07 15:25:14,317 ----------------------------------------------------------------------------------------------------
2024-09-07 15:25:14,333 Testing using last state of model ...
2024-09-07 15:25:20,967 
Results:
- F-score (micro) 0.6766
- F-score (macro) 0.4545
- Accuracy 0.5999

By class:
              precision    recall  f1-score   support

        PERS     0.8856    0.7890    0.8345      1678
         LOC     0.4434    0.7232    0.5498       401
         ORG     0.3179    0.6820    0.4336       261
        MISC     0.0000    0.0000    0.0000       240

   micro avg     0.6596    0.6946    0.6766      2580
   macro avg     0.4117    0.5486    0.4545      2580
weighted avg     0.6771    0.6946    0.6721      2580

2024-09-07 15:25:20,969 ----------------------------------------------------------------------------------------------------
