2024-09-07 17:17:26,628 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,631 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(119548, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0-5): 6 x TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
              (activation): GELUActivation()
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 128, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=256, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2024-09-07 17:17:26,633 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,635 Corpus: 7886 train + 876 dev + 4045 test sentences
2024-09-07 17:17:26,636 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,636 Train:  8762 sentences
2024-09-07 17:17:26,637         (train_with_dev=True, train_with_test=False)
2024-09-07 17:17:26,637 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,638 Training Params:
2024-09-07 17:17:26,639  - learning_rate: "0.01" 
2024-09-07 17:17:26,639  - mini_batch_size: "1"
2024-09-07 17:17:26,640  - max_epochs: "3"
2024-09-07 17:17:26,640  - shuffle: "True"
2024-09-07 17:17:26,641 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,642 Plugins:
2024-09-07 17:17:26,642  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2024-09-07 17:17:26,642 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,643 Final evaluation on model from best epoch (best-model.pt)
2024-09-07 17:17:26,644  - metric: "('micro avg', 'f1-score')"
2024-09-07 17:17:26,644 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,645 Computation:
2024-09-07 17:17:26,645  - compute on device: cuda:0
2024-09-07 17:17:26,646  - embedding storage: cpu
2024-09-07 17:17:26,646 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,647 Model training base path: "dist-finetuned.04"
2024-09-07 17:17:26,647 ----------------------------------------------------------------------------------------------------
2024-09-07 17:17:26,648 ----------------------------------------------------------------------------------------------------
2024-09-07 17:18:20,620 epoch 1 - iter 876/8762 - loss 0.30279056 - time (sec): 53.97 - samples/sec: 296.83 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:19:11,512 epoch 1 - iter 1752/8762 - loss 0.25733820 - time (sec): 104.86 - samples/sec: 304.33 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:20:01,684 epoch 1 - iter 2628/8762 - loss 0.22301044 - time (sec): 155.03 - samples/sec: 308.83 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:20:52,743 epoch 1 - iter 3504/8762 - loss 0.20248522 - time (sec): 206.09 - samples/sec: 311.77 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:21:42,048 epoch 1 - iter 4380/8762 - loss 0.18469125 - time (sec): 255.40 - samples/sec: 312.29 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:22:31,116 epoch 1 - iter 5256/8762 - loss 0.17801522 - time (sec): 304.47 - samples/sec: 313.31 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:23:21,057 epoch 1 - iter 6132/8762 - loss 0.17039668 - time (sec): 354.41 - samples/sec: 314.14 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:24:11,325 epoch 1 - iter 7008/8762 - loss 0.16674794 - time (sec): 404.68 - samples/sec: 315.11 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:25:01,419 epoch 1 - iter 7884/8762 - loss 0.16262862 - time (sec): 454.77 - samples/sec: 314.60 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:25:51,615 epoch 1 - iter 8760/8762 - loss 0.15869914 - time (sec): 504.97 - samples/sec: 314.26 - lr: 0.010000 - momentum: 0.000000
2024-09-07 17:25:51,780 ----------------------------------------------------------------------------------------------------
2024-09-07 17:25:51,781 EPOCH 1 done: loss 0.1587 - lr: 0.010000
